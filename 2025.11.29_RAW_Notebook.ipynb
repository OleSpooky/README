{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "cell_execution_strategy": "setup",
      "name": "2025.11.29_RAW_Notebook.ipynb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OleSpooky/README/blob/main/2025.11.29_RAW_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syw5vqAWU2rY"
      },
      "source": [
        "# Information Persistence Simulation - Interactive Notebook\n",
        "\n",
        "## Overview\n",
        "This notebook implements a vectorized Ising-like model for analyzing information persistence across coupled node networks. It explores how \"information pockets\" with strong coupling retain information longer than surrounding regions.\n",
        "\n",
        "## Environment Requirements\n",
        "\n",
        "### Python Dependencies\n",
        "All dependencies are managed in `requirements.txt`. Install via:\n",
        "```bash\n",
        "pip install -r requirements.txt\n",
        "```\n",
        "\n",
        "**Required versions:**\n",
        "- `numpy >= 1.20.0` - Uses `np.random.default_rng` API (introduced in NumPy 1.17)\n",
        "- `scipy >= 1.7.0` - For `scipy.special.expit` (sigmoid function)\n",
        "- `pandas >= 1.3.0` - For data analysis and CSV handling\n",
        "- `matplotlib >= 3.4.0` - For visualization\n",
        "- `ipywidgets >= 7.6.0` - For interactive sliders (compatible with @jupyter-widgets/controls v1.5.0)\n",
        "\n",
        "### Supported Environments\n",
        "- ✅ **Google Colab** (recommended for interactive widgets)\n",
        "- ✅ **Jupyter Notebook**\n",
        "- ✅ **JupyterLab**\n",
        "- ✅ **Local Python** (non-interactive mode)\n",
        "\n",
        "## Execution Order\n",
        "\n",
        "**For a complete workflow, run cells in this order:**\n",
        "\n",
        "1. **Cell 1**: Import dependencies and define simulation functions\n",
        "2. **Cell 3**: *(Optional)* Verify dependencies installed\n",
        "3. **Cell 4**: Run experiment runner to generate data (creates `colab_results/` folder)\n",
        "4. **Cell 5**: Combine CSV results and display summary\n",
        "5. **Cell 6**: Analyze persistence times\n",
        "6. **Cell 7**: Visualize total information flow\n",
        "7. **Cell 8-9**: *(Optional)* Mount Google Drive (Colab only)\n",
        "8. **Cell 13-14**: Interactive widget explorer (requires previous cells)\n",
        "\n",
        "## Key Features\n",
        "\n",
        "### 1. Vectorized Batch Simulation\n",
        "- Efficient parallel computation of M ensemble members\n",
        "- Memory-efficient chunking for large ensembles\n",
        "- Reproducible results with seed control\n",
        "\n",
        "### 2. Information Metrics\n",
        "- **Mutual Information I(A:X_i,t)**: Measures correlation between source and each node\n",
        "- **Persistence Time (τ)**: Time until MI drops below threshold\n",
        "- **Pocket Analysis**: Compare information retention inside vs. outside pockets\n",
        "\n",
        "### 3. Interactive Exploration\n",
        "- Real-time parameter tuning with sliders\n",
        "- Visual feedback: heatmaps, time series, and node-wise persistence\n",
        "- Parameter ranges: pocket strength (0-40), beta/inverse temperature (0.2-4.0)\n",
        "\n",
        "## Cross-Platform Compatibility\n",
        "\n",
        "- **File paths**: Uses `os.path.join()` for OS-agnostic path handling\n",
        "- **Folder creation**: Idempotent with `os.makedirs(folder, exist_ok=True)`\n",
        "- **Error handling**: CSV/NPY loading includes try-except blocks for robustness\n",
        "\n",
        "## Output Data\n",
        "\n",
        "Generated files are saved to `colab_results/`:\n",
        "- `tau_{config_name}.csv` - Persistence times for each node\n",
        "- `I_{config_name}.npy` - Mutual information matrix (T × N)\n",
        "\n",
        "---\n",
        "\n",
        "**Ready to begin? Run Cell 1 to load simulation functions.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2_33h9YkGQS"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# DEPENDENCIES AND IMPORTS\n",
        "# ==========================================\n",
        "# All required libraries are specified in requirements.txt\n",
        "# Install via: pip install -r requirements.txt\n",
        "# Core dependencies: numpy>=1.20.0, scipy>=1.7.0, pandas>=1.3.0\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "from scipy.special import expit  # Sigmoid function for probability calculations\n",
        "\n",
        "# ==========================================\n",
        "# PART 1: VECTORIZED SIMULATOR (The Engine)\n",
        "# ==========================================\n",
        "# This section implements a vectorized Ising-like model for information\n",
        "# persistence analysis across a network of nodes. The simulation tracks\n",
        "# how information propagates and decays through coupled nodes over time.\n",
        "\n",
        "def init_equilibrium_batch(N, M, rng):\n",
        "    \"\"\"\n",
        "    Initialize equilibrium states for a batch of simulations.\n",
        "\n",
        "    Args:\n",
        "        N (int): Number of nodes in the network\n",
        "        M (int): Ensemble size (number of parallel simulations)\n",
        "        rng: Numpy random generator\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Initial states (M x N) with random binary values\n",
        "    \"\"\"\n",
        "    return rng.integers(0, 2, size=(M, N), dtype=np.int8)\n",
        "\n",
        "def update_step_batch(X, c_arr, beta, theta_arr, rng):\n",
        "    \"\"\"\n",
        "    Update network states for one timestep using Ising-like dynamics.\n",
        "\n",
        "    This function computes the probability of each node being in state 1\n",
        "    based on its neighbors' states, coupling strengths, and temperature.\n",
        "\n",
        "    Args:\n",
        "        X (np.ndarray): Current states (M x N)\n",
        "        c_arr (np.ndarray): Coupling strengths between adjacent nodes (length N-1)\n",
        "        beta (float): Inverse temperature (controls noise/randomness)\n",
        "        theta_arr (np.ndarray): Bias/threshold for each node (length N)\n",
        "        rng: Numpy random generator\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Updated states (M x N)\n",
        "    \"\"\"\n",
        "    M, N = X.shape\n",
        "\n",
        "    # Calculate neighbor contributions (left and right neighbors)\n",
        "    left = np.zeros_like(X)\n",
        "    right = np.zeros_like(X)\n",
        "    left[:, 1:] = X[:, :-1] * c_arr[:N-1]   # Left neighbor influence\n",
        "    right[:, :-1] = X[:, 1:] * c_arr[:N-1]  # Right neighbor influence\n",
        "\n",
        "    # Compute total neighbor influence\n",
        "    neighbor_sum = left + right\n",
        "\n",
        "    # Calculate transition probability using sigmoid (expit)\n",
        "    bias = beta * (neighbor_sum - theta_arr[None, :])\n",
        "    p1 = expit(bias)  # Probability of being in state 1\n",
        "\n",
        "    # Stochastic update based on computed probabilities\n",
        "    U = rng.random(size=(M, N))\n",
        "    return (U < p1).astype(np.int8)\n",
        "\n",
        "def run_simulation_batch(N, c_arr, beta, theta_arr, M, T, source_j, master_seed=12345):\n",
        "    \"\"\"\n",
        "    Run batch simulation and collect statistics for mutual information calculation.\n",
        "\n",
        "    This is the main simulation driver that tracks co-occurrence counts between\n",
        "    the source node and all other nodes over time, which are used to compute\n",
        "    mutual information I(A:X_i,t).\n",
        "\n",
        "    Args:\n",
        "        N (int): Number of nodes\n",
        "        c_arr (np.ndarray): Coupling strengths (length N-1)\n",
        "        beta (float): Inverse temperature\n",
        "        theta_arr (np.ndarray): Node biases (length N)\n",
        "        M (int): Ensemble size (number of runs)\n",
        "        T (int): Number of timesteps\n",
        "        source_j (int): Index of the source node\n",
        "        master_seed (int): Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Co-occurrence counts (T x N x 2 x 2)\n",
        "                   Format: counts[t, node, source_state, node_state]\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(master_seed)\n",
        "\n",
        "    # Initialize count array for mutual information calculation\n",
        "    counts = np.zeros((T, N, 2, 2), dtype=np.int64)\n",
        "\n",
        "    # Initialize equilibrium states\n",
        "    X = init_equilibrium_batch(N, M, rng)\n",
        "\n",
        "    # Set source node to random binary value\n",
        "    A = rng.integers(0, 2, size=M, dtype=np.int8)\n",
        "    X[:, source_j] = A\n",
        "\n",
        "    # Run simulation and collect statistics\n",
        "    for t in range(T):\n",
        "        # Collect co-occurrence counts for this timestep\n",
        "        for a in (0, 1):\n",
        "            mask_a = (A == a)\n",
        "            if not np.any(mask_a):\n",
        "                continue\n",
        "            sub = X[mask_a]\n",
        "            ones = np.sum(sub, axis=0)\n",
        "            zeros = sub.shape[0] - ones\n",
        "            counts[t, :, a, 1] += ones\n",
        "            counts[t, :, a, 0] += zeros\n",
        "\n",
        "        # Update states for next timestep\n",
        "        X = update_step_batch(X, c_arr, beta, theta_arr, rng)\n",
        "\n",
        "    return counts\n",
        "\n",
        "def compute_mi_from_counts(counts, M):\n",
        "    \"\"\"\n",
        "    Compute mutual information I(A:X_i,t) from co-occurrence counts.\n",
        "\n",
        "    Mutual information quantifies how much knowing the source state tells\n",
        "    us about each node's state at each time. Higher MI indicates stronger\n",
        "    correlation and information retention.\n",
        "\n",
        "    Args:\n",
        "        counts (np.ndarray): Co-occurrence counts (T x N x 2 x 2)\n",
        "        M (int): Ensemble size (for normalization)\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Mutual information matrix (T x N)\n",
        "                   I[t, i] = I(A : X_i, t) in bits\n",
        "    \"\"\"\n",
        "    T, N, _, _ = counts.shape\n",
        "    I = np.zeros((T, N), dtype=float)\n",
        "    epsilon = 1e-12  # Small constant to avoid log(0)\n",
        "\n",
        "    for t in range(T):\n",
        "        for i in range(N):\n",
        "            # Compute joint probability p(a, x)\n",
        "            p_ax = counts[t, i].astype(float) / M\n",
        "\n",
        "            # Compute marginal probabilities\n",
        "            p_a = p_ax.sum(axis=1)  # p(source state)\n",
        "            p_x = p_ax.sum(axis=0)  # p(node state)\n",
        "\n",
        "            # Calculate mutual information using I(A:X) = Σ p(a,x) log[p(a,x)/(p(a)p(x))]\n",
        "            mi = 0.0\n",
        "            for a_val in (0, 1):\n",
        "                for x_val in (0, 1):\n",
        "                    p = p_ax[a_val, x_val]\n",
        "                    if p > epsilon:\n",
        "                        denom = p_a[a_val] * p_x[x_val]\n",
        "                        if denom > epsilon:\n",
        "                            mi += p * np.log2(p / denom)\n",
        "            I[t, i] = mi\n",
        "\n",
        "    return I\n",
        "\n",
        "def compute_tau(I_matrix, threshold=1e-2):\n",
        "    \"\"\"\n",
        "    Compute persistence time (tau) for each node.\n",
        "\n",
        "    Tau is defined as the first timestep where mutual information drops\n",
        "    below the threshold, indicating information loss/decay.\n",
        "\n",
        "    Args:\n",
        "        I_matrix (np.ndarray): Mutual information over time (T x N)\n",
        "        threshold (float): MI threshold for considering information lost\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Persistence times for each node (length N)\n",
        "                   tau[i] = first time where I[t,i] < threshold\n",
        "    \"\"\"\n",
        "    T, N = I_matrix.shape\n",
        "    taus = np.zeros(N, dtype=int)\n",
        "\n",
        "    for i in range(N):\n",
        "        # Find first timestep where MI drops below threshold\n",
        "        decayed = np.where(I_matrix[:, i] < threshold)[0]\n",
        "        if len(decayed) > 0:\n",
        "            taus[i] = decayed[0]\n",
        "        else:\n",
        "            taus[i] = T  # Information never decayed within simulation time\n",
        "\n",
        "    return taus\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "St84OAa4Vm3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45ce3300"
      },
      "source": [
        "# Dependencies are managed in requirements.txt\n",
        "# Install via: pip install -r requirements.txt\n",
        "# Required: numpy>=1.20.0, scipy>=1.7.0, pandas>=1.3.0, matplotlib>=3.4.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# PART 2: EXPERIMENT RUNNER (Data Generation)\n",
        "# ==========================================\n",
        "\n",
        "# Settings\n",
        "N = 21\n",
        "M = 2000        # Number of runs per batch\n",
        "T = 60          # Timesteps\n",
        "source_j = 10   # Source in the middle\n",
        "beta = 2.0\n",
        "theta_arr = np.ones(N) * 1.0\n",
        "folder = 'colab_results'\n",
        "\n",
        "# Ensure clean start\n",
        "# Create folder if it doesn't exist (idempotent)\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "# Define 3 scenarios to demonstrate the contrast\n",
        "scenarios = [\n",
        "    (\"homogenous\", np.ones(N-1) * 1.0),             # Baseline\n",
        "    (\"p10.0_weak\",  np.ones(N-1) * 1.0),            # Will modify below\n",
        "    (\"p10.0_strong\", np.ones(N-1) * 1.0)            # Will modify below\n",
        "]\n",
        "\n",
        "# Apply modifications for pockets\n",
        "# Pocket at index 10 means couplings (9-10) and (10-11) are strong\n",
        "c_weak = np.ones(N-1) * 1.0\n",
        "c_weak[9:11] = 3.0  # Weak pocket\n",
        "scenarios[1] = (\"beta2.0_p10.0_weak\", c_weak)\n",
        "\n",
        "c_strong = np.ones(N-1) * 1.0\n",
        "c_strong[9:11] = 5.0 # Strong pocket\n",
        "scenarios[2] = (\"beta2.0_p10.0_strong\", c_strong)\n",
        "\n",
        "print(f\"Running {len(scenarios)} simulations...\")\n",
        "\n",
        "for name, c_profile in scenarios:\n",
        "    print(f\"  -> Simulating: {name}\")\n",
        "    counts = run_simulation_batch(N, c_profile, beta, theta_arr, M, T, source_j)\n",
        "    I = compute_mi_from_counts(counts, M)\n",
        "    taus = compute_tau(I, threshold=0.01)\n",
        "\n",
        "    # Save the I array for further analysis\n",
        "    np.save(os.path.join(folder, f'I_{name}.npy'), I)\n",
        "\n",
        "    # Save to CSV in the format expected by your analysis script\n",
        "    # Structure: node_index, tau\n",
        "    df = pd.DataFrame({\n",
        "        'node_index': np.arange(N),\n",
        "        'tau': taus\n",
        "    })\n",
        "    filename = os.path.join(folder, f'tau_{name}.csv')\n",
        "    df.to_csv(filename, index=False)\n",
        "\n",
        "print(\"Data generation complete.\\n\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "098zuvZ5klN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4966a263"
      },
      "source": [
        "# Paste and run: combine all tau CSVs into one DataFrame and show summary\n",
        "import os, glob, pandas as pd, numpy as np\n",
        "\n",
        "folder = 'colab_results'\n",
        "\n",
        "# Create folder if it doesn't exist\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "files = sorted(glob.glob(os.path.join(folder, 'tau_*.csv')))\n",
        "dfs = []\n",
        "\n",
        "# Process each CSV with error handling for malformed files\n",
        "for f in files:\n",
        "    try:\n",
        "        cfg = os.path.basename(f).replace('tau_','').replace('.csv','')\n",
        "        df = pd.read_csv(f)\n",
        "        df['config'] = cfg\n",
        "        dfs.append(df)\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not read {f}: {e}\")\n",
        "        continue\n",
        "\n",
        "if dfs:\n",
        "    all_tau = pd.concat(dfs, ignore_index=True)\n",
        "    display(all_tau.head(40))\n",
        "    # compute mean tau inside/outside pocket if pockets exist\n",
        "    print(f\"\\nSummary: {len(dfs)} CSV files combined, {len(all_tau)} total samples\")\n",
        "else:\n",
        "    print(\"No CSV files found in\", folder)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58036afd"
      },
      "source": [
        "# ==========================================\n",
        "# PART 3: YOUR ANALYSIS SCRIPT (Data Processing)\n",
        "# ==========================================\n",
        "# This cell analyzes persistence times from generated CSV files\n",
        "# and visualizes information retention across different network configurations\n",
        "\n",
        "print(\"--- RUNNING USER ANALYSIS ---\")\n",
        "\n",
        "# Read all tau CSV files with error handling\n",
        "files = sorted(glob.glob(os.path.join(folder, 'tau_*.csv')))\n",
        "dfs = []\n",
        "\n",
        "for f in files:\n",
        "    try:\n",
        "        # Extract config name from filename (e.g., 'tau_homogenous.csv' -> 'homogenous')\n",
        "        cfg = os.path.basename(f).replace('tau_', '').replace('.csv', '')\n",
        "        df = pd.read_csv(f)\n",
        "        df['config'] = cfg\n",
        "        dfs.append(df)\n",
        "        print(f\"✓ Loaded {cfg}: {len(df)} samples\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Warning: Could not read {os.path.basename(f)}: {e}\")\n",
        "        continue\n",
        "\n",
        "if dfs:\n",
        "    # Combine all dataframes\n",
        "    all_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    # Display summary statistics\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Summary: {len(dfs)} configurations, {len(all_df)} total samples\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Group by configuration and compute statistics\n",
        "    summary = all_df.groupby('config')['tau'].agg(['mean', 'std', 'min', 'max', 'count'])\n",
        "    print(summary)\n",
        "else:\n",
        "    print(f\"⚠ No valid CSV files found in '{folder}'. Run data generation first.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a46d694"
      },
      "source": [
        "# ==========================================\n",
        "# ANALYSIS: Visualize Total Information Flow\n",
        "# ==========================================\n",
        "# Compute and plot Σ_i I(A:i,t) - the sum of mutual information\n",
        "# across all nodes over time. This shows total information retention.\n",
        "\n",
        "import os, glob, numpy as np, matplotlib.pyplot as plt\n",
        "\n",
        "folder = 'colab_results'\n",
        "npy_files = sorted(glob.glob(os.path.join(folder, 'I_*.npy')))\n",
        "\n",
        "if npy_files:\n",
        "    plt.figure(figsize=(8, 5))\n",
        "\n",
        "    for f in npy_files:\n",
        "        try:\n",
        "            # Load mutual information matrix (shape: T x N)\n",
        "            I = np.load(f)\n",
        "\n",
        "            # Sum across all nodes to get total information at each timestep\n",
        "            s = I.sum(axis=1)\n",
        "\n",
        "            # Extract label from filename for plot legend\n",
        "            label = os.path.basename(f).replace('I_', '').replace('.npy', '')\n",
        "            plt.plot(s, label=label, linewidth=2)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Warning: Could not load {os.path.basename(f)}: {e}\")\n",
        "            continue\n",
        "\n",
        "    plt.xlabel('Timestep', fontsize=12)\n",
        "    plt.ylabel('Sum I(A:i,t)', fontsize=12)\n",
        "    plt.title('Total Information Retention Over Time', fontsize=14)\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(f\"⚠ No NPY files found in '{folder}'. Run data generation first.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c729177c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9X_TJo3U2ro"
      },
      "source": [
        "## Google Drive Integration (Colab Only)\n",
        "\n",
        "After running the above cell and following the authentication steps, your Google Drive will be mounted at `/content/drive`.\n",
        "\n",
        "**Access patterns:**\n",
        "```python\n",
        "# Example: Save to Drive\n",
        "import pandas as pd\n",
        "df.to_csv('/content/drive/My Drive/colab_results/data.csv')\n",
        "\n",
        "# Example: Load from Drive\n",
        "data = pd.read_csv('/content/drive/My Drive/your_folder/your_file.csv')\n",
        "```\n",
        "\n",
        "**Note:** This cell only works in Google Colab. For local Jupyter, save files to the local filesystem instead.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f63fb35"
      },
      "source": [
        "# Widget dependencies are managed in requirements.txt\n",
        "# Install via: pip install -r requirements.txt\n",
        "# Required: ipywidgets>=7.6.0 (@jupyter-widgets/controls v1.5.0 compatible)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8f09928"
      },
      "source": [
        "# ==========================================\n",
        "# PART 2: EXPERIMENT RUNNER (Data Generation)\n",
        "# ==========================================\n",
        "\n",
        "# Settings\n",
        "N = 21\n",
        "M = 2000        # Number of runs per batch\n",
        "T = 60          # Timesteps\n",
        "source_j = 10   # Source in the middle\n",
        "beta = 2.0\n",
        "theta_arr = np.ones(N) * 1.0\n",
        "folder = 'colab_results'\n",
        "\n",
        "# Ensure clean start\n",
        "# Create folder if it doesn't exist (idempotent)\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "# Define 3 scenarios to demonstrate the contrast\n",
        "scenarios = [\n",
        "    (\"homogenous\", np.ones(N-1) * 1.0),             # Baseline\n",
        "    (\"p10.0_weak\",  np.ones(N-1) * 1.0),            # Will modify below\n",
        "    (\"p10.0_strong\", np.ones(N-1) * 1.0)            # Will modify below\n",
        "]\n",
        "\n",
        "# Apply modifications for pockets\n",
        "# Pocket at index 10 means couplings (9-10) and (10-11) are strong\n",
        "c_weak = np.ones(N-1) * 1.0\n",
        "c_weak[9:11] = 3.0  # Weak pocket\n",
        "scenarios[1] = (\"beta2.0_p10.0_weak\", c_weak)\n",
        "\n",
        "c_strong = np.ones(N-1) * 1.0\n",
        "c_strong[9:11] = 5.0 # Strong pocket\n",
        "scenarios[2] = (\"beta2.0_p10.0_strong\", c_strong)\n",
        "\n",
        "print(f\"Running {len(scenarios)} simulations...\")\n",
        "\n",
        "for name, c_profile in scenarios:\n",
        "    print(f\"  -> Simulating: {name}\")\n",
        "    counts = run_simulation_batch(N, c_profile, beta, theta_arr, M, T, source_j)\n",
        "    I = compute_mi_from_counts(counts, M)\n",
        "    taus = compute_tau(I, threshold=0.01)\n",
        "\n",
        "    # Save the I array for further analysis\n",
        "    np.save(os.path.join(folder, f'I_{name}.npy'), I)\n",
        "\n",
        "    # Save to CSV in the format expected by your analysis script\n",
        "    # Structure: node_index, tau\n",
        "    df = pd.DataFrame({\n",
        "        'node_index': np.arange(N),\n",
        "        'tau': taus\n",
        "    })\n",
        "    filename = os.path.join(folder, f'tau_{name}.csv')\n",
        "    df.to_csv(filename, index=False)\n",
        "\n",
        "print(\"Data generation complete.\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# PART 3: YOUR ANALYSIS SCRIPT (Data Processing)\n",
        "# ==========================================\n",
        "# This cell analyzes persistence times from generated CSV files\n",
        "# and visualizes information retention across different network configurations\n",
        "\n",
        "print(\"--- RUNNING USER ANALYSIS ---\")\n",
        "\n",
        "# Read all tau CSV files with error handling\n",
        "files = sorted(glob.glob(os.path.join(folder, 'tau_*.csv')))\n",
        "dfs = []\n",
        "\n",
        "for f in files:\n",
        "    try:\n",
        "        # Extract config name from filename (e.g., 'tau_homogenous.csv' -> 'homogenous')\n",
        "        cfg = os.path.basename(f).replace('tau_', '').replace('.csv', '')\n",
        "        df = pd.read_csv(f)\n",
        "        df['config'] = cfg\n",
        "        dfs.append(df)\n",
        "        print(f\"✓ Loaded {cfg}: {len(df)} samples\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Warning: Could not read {os.path.basename(f)}: {e}\")\n",
        "        continue\n",
        "\n",
        "if dfs:\n",
        "    # Combine all dataframes\n",
        "    all_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    # Display summary statistics\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Summary: {len(dfs)} configurations, {len(all_df)} total samples\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Group by configuration and compute statistics\n",
        "    summary = all_df.groupby('config')['tau'].agg(['mean', 'std', 'min', 'max', 'count'])\n",
        "    print(summary)\n",
        "else:\n",
        "    print(f\"⚠ No valid CSV files found in '{folder}'. Run data generation first.\")\n"
      ],
      "metadata": {
        "id": "glxpJek5klmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# INTERACTIVE WIDGET EXPLORER\n",
        "# ==========================================\n",
        "# This cell provides an interactive interface for exploring parameter space\n",
        "# using ipywidgets. Requires: ipywidgets>=7.6.0 (@jupyter-widgets/controls v1.5.0)\n",
        "#\n",
        "# ENVIRONMENT NOTES:\n",
        "# - Works in Jupyter Notebook, JupyterLab, and Google Colab\n",
        "# - Widget version compatibility: @jupyter-widgets/controls 1.5.0\n",
        "# - NumPy version: >=1.20.0 (for np.random.default_rng API)\n",
        "# - SciPy version: >=1.7.0 (for expit function)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from ipywidgets import interact, IntSlider, FloatSlider\n",
        "import time\n",
        "from scipy.special import expit\n",
        "\n",
        "# ==========================================\n",
        "# Core Simulation Functions (Vectorized Batch)\n",
        "# ==========================================\n",
        "# These functions are duplicated here to make the widget cell self-contained\n",
        "\n",
        "def init_equilibrium_batch(N, M, rng):\n",
        "    \"\"\"Initialize random binary states for batch simulation.\"\"\"\n",
        "    return rng.integers(0, 2, size=(M, N), dtype=np.int8)\n",
        "\n",
        "def update_step_batch(X, c_arr, beta, theta_arr, rng):\n",
        "    \"\"\"\n",
        "    Perform one timestep update using Ising-like dynamics.\n",
        "    Note: c_arr should have length N-1 (coupling between adjacent nodes).\n",
        "    \"\"\"\n",
        "    M, N = X.shape\n",
        "    left = np.zeros_like(X)\n",
        "    right = np.zeros_like(X)\n",
        "    left[:, 1:] = X[:, :-1] * c_arr\n",
        "    right[:, :-1] = X[:, 1:] * c_arr\n",
        "    neighbor_sum = left + right\n",
        "    bias = beta * (neighbor_sum - theta_arr[None, :])\n",
        "    p1 = expit(bias)\n",
        "    U = rng.random(size=(M, N))\n",
        "    X_new = (U < p1).astype(np.int8)\n",
        "    return X_new\n",
        "\n",
        "def compute_mutual_information_from_counts(counts, M):\n",
        "    \"\"\"Calculate mutual information I(A:X_i,t) from co-occurrence counts.\"\"\"\n",
        "    T, N, _, _ = counts.shape\n",
        "    I = np.zeros((T, N), dtype=float)\n",
        "    epsilon = 1e-12  # Avoid log(0)\n",
        "\n",
        "    for t in range(T):\n",
        "        for i in range(N):\n",
        "            p_a_x = counts[t, i].astype(float) / M\n",
        "            p_a = p_a_x.sum(axis=1)\n",
        "            p_x = p_a_x.sum(axis=0)\n",
        "            mi = 0.0\n",
        "            for a in (0, 1):\n",
        "                for x in (0, 1):\n",
        "                    p = p_a_x[a, x]\n",
        "                    if p <= epsilon:\n",
        "                        continue\n",
        "                    denom = p_a[a] * p_x[x]\n",
        "                    if denom <= epsilon:\n",
        "                        continue\n",
        "                    mi += p * np.log2(p / denom)\n",
        "            I[t, i] = mi\n",
        "    return I\n",
        "\n",
        "def persistence_times(I_arr, eps):\n",
        "    \"\"\"\n",
        "    Compute persistence time (tau) for each node.\n",
        "    Returns first timestep where I drops below threshold eps.\n",
        "    \"\"\"\n",
        "    T, N = I_arr.shape\n",
        "    tau = np.full(N, T, dtype=int)\n",
        "    for i in range(N):\n",
        "        below_indices = np.where(I_arr[:, i] < eps)[0]\n",
        "        if below_indices.size > 0:\n",
        "            tau[i] = below_indices[0]\n",
        "        else:\n",
        "            tau[i] = T  # Never decayed within T timesteps\n",
        "    return tau\n",
        "\n",
        "def run_simulation_chunked(N, c_arr, beta, theta_arr, M, T, source_j,\n",
        "                           master_seed=12345, chunk_size=500, continuous_source=False):\n",
        "    \"\"\"\n",
        "    Run batch simulation with memory-efficient chunking.\n",
        "    Processes M simulations in chunks to avoid memory overflow.\n",
        "    \"\"\"\n",
        "    main_rng = np.random.default_rng(master_seed)\n",
        "    total_counts = np.zeros((T, N, 2, 2), dtype=np.int64)\n",
        "\n",
        "    num_chunks = M // chunk_size + (1 if M % chunk_size != 0 else 0)\n",
        "\n",
        "    for chunk_idx in range(num_chunks):\n",
        "        current_chunk_size = min(chunk_size, M - chunk_idx * chunk_size)\n",
        "        if current_chunk_size == 0:\n",
        "            continue\n",
        "\n",
        "        # Generate unique seed for reproducibility\n",
        "        chunk_seed = main_rng.integers(0, 2**32 - 1)\n",
        "        chunk_rng = np.random.default_rng(chunk_seed)\n",
        "\n",
        "        X = init_equilibrium_batch(N, current_chunk_size, chunk_rng)\n",
        "        A = chunk_rng.integers(0, 2, size=current_chunk_size, dtype=np.int8)\n",
        "        X[:, source_j] = A\n",
        "\n",
        "        for t in range(T):\n",
        "            # Accumulate co-occurrence counts\n",
        "            for a in (0, 1):\n",
        "                mask_a = (A == a)\n",
        "                sub = X[mask_a]\n",
        "                if sub.size > 0:\n",
        "                    total_counts[t, :, a, 1] += np.sum(sub, axis=0)\n",
        "                    total_counts[t, :, a, 0] += sub.shape[0] - np.sum(sub, axis=0)\n",
        "\n",
        "            X = update_step_batch(X, c_arr, beta, theta_arr, chunk_rng)\n",
        "\n",
        "            if continuous_source:\n",
        "                X[:, source_j] = A  # Continuous source clamping (if enabled)\n",
        "\n",
        "    return total_counts\n",
        "\n",
        "# ==========================================\n",
        "# Interactive Visualization Function\n",
        "# ==========================================\n",
        "\n",
        "def interactive_two(pocket_strength=10, beta=1.5):\n",
        "    \"\"\"\n",
        "    Interactive explorer for pocket strength and temperature (beta) parameters.\n",
        "\n",
        "    Parameters:\n",
        "        pocket_strength: Coupling strength in the pocket region (0-40)\n",
        "        beta: Inverse temperature controlling noise/randomness (0.2-4.0)\n",
        "    \"\"\"\n",
        "    # Simulation parameters\n",
        "    N = 21              # Number of nodes\n",
        "    T = 60              # Timesteps\n",
        "    source_j = 10       # Source node (center)\n",
        "    M = 1000            # Ensemble size (keep moderate for interactivity)\n",
        "    chunk_size = 500    # Chunk size for memory efficiency\n",
        "    theta = 1.0         # Node bias/threshold\n",
        "    continuous_source = False  # Source drives once at t=0\n",
        "    eps = 1e-3          # Threshold for tau calculation\n",
        "\n",
        "    # Create coupling array with pocket in the center\n",
        "    c_pocket = np.ones(N-1)  # Base coupling = 1.0\n",
        "    mid = N // 2             # Center index\n",
        "    half = 3                 # Pocket half-width\n",
        "    # Apply strong coupling in pocket region (nodes mid-half to mid+half-1)\n",
        "    c_pocket[mid-half:mid+half] = float(pocket_strength)\n",
        "\n",
        "    # Run simulation\n",
        "    t0 = time.perf_counter()\n",
        "    counts = run_simulation_chunked(N, c_pocket, float(beta), np.ones(N)*theta,\n",
        "                                    M, T, source_j, master_seed=20251123,\n",
        "                                    chunk_size=chunk_size, continuous_source=continuous_source)\n",
        "    I = compute_mutual_information_from_counts(counts, M)\n",
        "    tau = persistence_times(I, eps=eps)\n",
        "    elapsed = time.perf_counter() - t0\n",
        "\n",
        "    # ==========================================\n",
        "    # Visualization: 3-panel figure\n",
        "    # ==========================================\n",
        "    plt.figure(figsize=(10, 3))\n",
        "\n",
        "    # Panel 1: Total information retention over time\n",
        "    plt.subplot(1, 3, 1)\n",
        "    S = I.sum(axis=1)  # Sum MI across all nodes\n",
        "    plt.plot(S, '-o', markersize=3)\n",
        "    plt.xlabel('Timestep')\n",
        "    plt.ylabel('Sum I(A:i,t)')\n",
        "    plt.title('Global Information Retention')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Panel 2: Heatmap of MI(t, node)\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(I.T, origin='lower', aspect='auto', cmap='viridis', extent=[0, T, 0, N])\n",
        "    plt.title(f'MI Heatmap (pocket={pocket_strength}, β={beta})')\n",
        "    plt.xlabel('Timestep')\n",
        "    plt.ylabel('Node index')\n",
        "    plt.colorbar(shrink=0.6, label='I(A:X_i)')\n",
        "\n",
        "    # Panel 3: Persistence times per node\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(tau, marker='o', linewidth=2, markersize=5)\n",
        "    # Highlight pocket region with shaded area\n",
        "    plt.axvspan(mid-half, mid+half-1, color='orange', alpha=0.2, label='Pocket')\n",
        "    plt.xlabel('Node index')\n",
        "    plt.ylabel('Persistence time (tau)')\n",
        "    plt.title('Information Persistence per Node')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print summary statistics\n",
        "    inside_idx = list(range(mid-half, mid+half))  # Pocket nodes\n",
        "    outside_idx = [i for i in range(N) if i not in inside_idx]  # Non-pocket nodes\n",
        "\n",
        "    mean_inside = np.nanmean(tau[inside_idx])\n",
        "    mean_outside = np.nanmean(tau[outside_idx])\n",
        "\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Simulation Results:\")\n",
        "    print(f\"  Pocket strength: {pocket_strength}\")\n",
        "    print(f\"  Beta (inv. temp): {beta}\")\n",
        "    print(f\"  Mean tau (inside pocket): {mean_inside:.2f} timesteps\")\n",
        "    print(f\"  Mean tau (outside pocket): {mean_outside:.2f} timesteps\")\n",
        "    print(f\"  Computation time: {elapsed:.2f}s\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "# ==========================================\n",
        "# Widget Configuration and Display\n",
        "# ==========================================\n",
        "# Create sliders with descriptive parameters\n",
        "p_slider = IntSlider(\n",
        "    value=10,\n",
        "    min=0,\n",
        "    max=40,\n",
        "    step=1,\n",
        "    description='Pocket Strength',\n",
        "    style={'description_width': '120px'}\n",
        ")\n",
        "\n",
        "b_slider = FloatSlider(\n",
        "    value=1.5,\n",
        "    min=0.2,\n",
        "    max=4.0,\n",
        "    step=0.1,\n",
        "    description='Beta (inv. temp)',\n",
        "    style={'description_width': '120px'}\n",
        ")\n",
        "\n",
        "# Initialize interactive widget\n",
        "# Note: This will only work in Jupyter environments (Notebook, Lab, Colab)\n",
        "print(\"Initializing interactive widget explorer...\")\n",
        "print(\"Adjust sliders to explore parameter space\\n\")\n",
        "interact(interactive_two, pocket_strength=p_slider, beta=b_slider)\n"
      ],
      "metadata": {
        "id": "PNaIPY6Nw10J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# INTERACTIVE WIDGET - COMPACT VERSION\n",
        "# ==========================================\n",
        "# This is a streamlined version of the interactive explorer.\n",
        "# For the full version with detailed comments, see Cell 13.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from ipywidgets import interact, IntSlider, FloatSlider\n",
        "import time\n",
        "\n",
        "def interactive_two(pocket_strength=10, beta=1.5):\n",
        "    \"\"\"Interactive explorer for pocket strength and beta parameters.\"\"\"\n",
        "    N = 21\n",
        "    T = 60\n",
        "    source_j = 10\n",
        "    M = 1000\n",
        "    chunk_size = 500\n",
        "    theta = 1.0\n",
        "    continuous_source = False\n",
        "    eps = 1e-3\n",
        "\n",
        "    # Build pocket coupling\n",
        "    c_pocket = np.ones(N-1)\n",
        "    mid = N // 2\n",
        "    half = 3\n",
        "    c_pocket[mid-half:mid+half] = float(pocket_strength)\n",
        "\n",
        "    # Run simulation\n",
        "    t0 = time.perf_counter()\n",
        "    counts = run_simulation_chunked(N, c_pocket, float(beta), np.ones(N)*theta,\n",
        "                                    M, T, source_j, master_seed=20251123,\n",
        "                                    chunk_size=chunk_size, continuous_source=continuous_source)\n",
        "    I = compute_mutual_information_from_counts(counts, M)\n",
        "    tau = persistence_times(I, eps=eps)\n",
        "    elapsed = time.perf_counter() - t0\n",
        "\n",
        "    # Visualize results\n",
        "    S = I.sum(axis=1)\n",
        "    plt.figure(figsize=(10, 3))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(S, '-o', markersize=3)\n",
        "    plt.xlabel('Timestep')\n",
        "    plt.ylabel('Sum I(A:i,t)')\n",
        "    plt.title('Global Information Retention')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(I.T, origin='lower', aspect='auto', cmap='viridis', extent=[0, T, 0, N])\n",
        "    plt.title(f'I heatmap (p={pocket_strength}, β={beta})')\n",
        "    plt.xlabel('Timestep')\n",
        "    plt.ylabel('Node index')\n",
        "    plt.colorbar(shrink=0.6)\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(tau, marker='o')\n",
        "    plt.axvspan(mid-half, mid+half-1, color='orange', alpha=0.2)\n",
        "    plt.xlabel('Node index')\n",
        "    plt.ylabel('Persistence time')\n",
        "    plt.title('tau per node')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    inside_idx = list(range(mid-half, mid+half))\n",
        "    outside_idx = [i for i in range(N) if i not in inside_idx]\n",
        "    mean_inside = np.nanmean(tau[inside_idx])\n",
        "    mean_outside = np.nanmean(tau[outside_idx])\n",
        "    print(f\"pocket={pocket_strength} β={beta}  τ_in={mean_inside:.2f}  τ_out={mean_outside:.2f}  time={elapsed:.2f}s\")\n",
        "\n",
        "# Create sliders\n",
        "p_slider = IntSlider(value=10, min=0, max=40, step=1, description='pocket')\n",
        "b_slider = FloatSlider(value=1.5, min=0.2, max=4.0, step=0.1, description='beta')\n",
        "\n",
        "# Initialize widget\n",
        "interact(interactive_two, pocket_strength=p_slider, beta=b_slider)\n"
      ],
      "metadata": {
        "id": "AbnliOBtxUaR",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DEzjUN-I-hP4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}